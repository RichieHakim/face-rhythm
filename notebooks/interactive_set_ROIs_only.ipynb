{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set ROIs\n",
    "\n",
    "This notebook allows you to make ROIs for a video without going through the entire pipeline. The example frame used as the image to set the ROIs is fetched using OpenCV's video reader.\\\n",
    "The functionality of this notebook is technically subsumed by the basic pipeline notebook.\n",
    "\n",
    "**If your data is local**: Just specify the path to the video you want to draw ROIs on.\n",
    "\n",
    "**If your data is on a server**: OpenCV's video reader allows for you to read just a single frame from a video file without transferring the entire thing IF the server directory is mounted. So we recommend mounting the server directory if possible to avoid transferring the entire file. This can be done with:\n",
    "- Windows: https://support.microsoft.com/en-us/windows/map-a-network-drive-in-windows-29ce55d1-34e3-a7e2-4801-131475f9557d\n",
    "- OSX: https://www.google.com/search?q=mount+network+drive+osx\n",
    "- Linux: Use RClone CLI: `rclone mount remote:path/to/files /path/to/local/mount`\n",
    "    - Example: `rclone mount transfer:/n/files/Neurobio/MICROSCOPE/ /mnt/MICROSCOPE/`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "== Operating System ==: uname_result(system='Linux', node='rich-L-CLASS', release='5.15.0-76-generic', version='#83~20.04.1-Ubuntu SMP Wed Jun 21 20:23:31 UTC 2023', machine='x86_64')\n",
      "== CPU Info ==: {'n_cores': 36, 'brand': 'Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz'}\n",
      "== RAM ==: svmem(total=269925031936, available=243630243840, percent=9.7, used=23660290048, free=75224793088, active=80180273152, inactive=109870620672, buffers=80977772544, cached=90062176256, shared=332603392, slab=3671760896)\n",
      "== GPU Info ==: {0: {'id': 0, 'uuid': 'GPU-361d808e-1136-2078-83bf-4290bf948f25', 'load': 0.32, 'memoryUtil': 0.050577799479166664, 'memoryTotal': 24576.0, 'memoryUsed': 1243.0, 'memoryFree': 22983.0, 'driver': '525.125.06', 'name': 'NVIDIA GeForce RTX 3090', 'serial': '1324220011997', 'display_mode': 'Enabled', 'display_active': 'Enabled', 'temperature': 47.0}}\n",
      "== Conda Environment ==: FR\n",
      "== Python Version ==: 3.9.16\n",
      "== GCC Version ==: 9.4.0\n",
      "== PyTorch Version ==: 2.0.1+cu117\n",
      "== CUDA Version ==: 11.7, CUDNN Version: 8500, Number of Devices: 1, Devices: ['device 0: Name=NVIDIA GeForce RTX 3090, Memory=25.403129856 GB'], \n",
      "== face_rhythm Version ==: <module 'face_rhythm' from '/media/rich/Home_Linux_partition/github_repos/face-rhythm/face_rhythm/__init__.py'>\n",
      "== face_rhythm date installed ==: Sun Jul  2 18:57:27 2023\n"
     ]
    }
   ],
   "source": [
    "# ALWAYS RUN THIS CELL\n",
    "# widen jupyter notebook window\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import face_rhythm as fr\n",
    "\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot\n",
    "from tqdm import tqdm\n",
    "\n",
    "fr.util.system_info(verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_videos  = '/mnt/MICROSCOPE/Gyu/BMI_data/cage_0322/'\n",
    "filename_strMatch = '.*/2023.*cam4.*avi'  ## You can use regular expressions to search and match more complex strings\n",
    "# filename_strMatch = 'cam4.*avi'  ## You can use regular expressions to search and match more complex strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_videos = fr.helpers.find_paths(\n",
    "    dir_outer=directory_videos,\n",
    "    reMatch=filename_strMatch,  ## string to use to search for files in directory. Uses regular expressions!\n",
    "    depth=5,  ## how many folders deep to search\n",
    ")\n",
    "\n",
    "display(paths_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_save = r'/home/rich/Desktop/0322N_and_0322R/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = cv2.VideoCapture(paths_videos[0])\n",
    "s, image = vc.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either select new ROIs (`select_mode='gui'`), or import existing ROIs (`path_file=path_to_ROIs.h5_file`).\\\n",
    "Typically, you should make 1 or 2 ROIs. One for defining where the face points should be and one for cropping the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "rois = fr.rois.ROIs(\n",
    "    select_mode='gui',\n",
    "    exampleImage=image,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the `ROIs` object in the 'analysis_files' project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois.make_points(rois[0], point_spacing=9)\n",
    "rois.plot_rois()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_save = str(Path(directory_save) / 'ROIs.h5')\n",
    "# rois.save_run_data(path_run_data=path_save, overwrite=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Multisession alignment\n",
    "\n",
    "The below code shows how to align the points from one 'template' session onto multiple other 'new' sessions.\\\n",
    "Steps:\n",
    "1. Get example images from each session: `images`\n",
    "2. Optionally adjust the local contrast of each example image to make: `images_toUse`\n",
    "3. Instantiate the `ROI_Aligner` class and choose which OpenCV optical flow method to use: `aligner`\n",
    "4. Perform non-rigid registration to warp the 'template' ROIs and points onto the example images from each session: `aligner.align_and_make_ROIs`\n",
    "5. Retrieve the newly made `ROIs` class objects: `rois_objs_new = aligner.ROIs_objects_new`\n",
    "6. Save the new `ROIs` class objects: `rois_objs_new[x].save_run_data()`\n",
    "7. Visualize the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_first_frames(paths, max_attempts=5, verbose=True):\n",
    "    if isinstance(paths, list) == False:\n",
    "        paths = [paths]\n",
    "\n",
    "    def get_first_frame(path, attempt=0):\n",
    "        frame = cv2.VideoCapture(path).read()[1]\n",
    "        if frame is None:\n",
    "            print(f\"Received `None` for path: {path}. Attempting again; attempt number: {attempt}\")\n",
    "            if attempt + 1 > max_attempts:\n",
    "                warnings.warn(f\"Failed to get first frame for path:{path}\")\n",
    "                return None\n",
    "            frame = get_first_frame(path, attempt=attempt+1)\n",
    "        return frame\n",
    "\n",
    "    flag_singleOutput = len(paths) == 1\n",
    "    out = [get_first_frame(path) for path in tqdm(paths, disable=flag_singleOutput)]\n",
    "    out = out[0] if flag_singleOutput else out\n",
    "    \n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {path: get_first_frames(path) for path in tqdm(paths_videos)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Optionally adjust local contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. to 5. Register ROIs from template to new session images, then make new `ROIs` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = fr.rois.Image_Aligner(verbose=True)\n",
    "\n",
    "ims_augmented = aligner.augment_images(\n",
    "    ims=list(images.values()),\n",
    "    use_CLAHE=True,\n",
    "    CLAHE_grid_size=5,\n",
    "    CLAHE_clipLimit=1,\n",
    "    CLAHE_normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.visualization.display_toggle_image_stack(ims_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_template = rois.exampleImage.copy()\n",
    "\n",
    "aligner.fit_geometric(\n",
    "    template=im_template,\n",
    "    ims_moving=ims_augmented,  ## input images\n",
    "    template_method='image',  ## 'sequential': align images to neighboring images (good for drifting data). 'image': align to a single image\n",
    "    mode_transform='euclidean',  ## type of geometric transformation. See openCV's cv2.findTransformECC for details\n",
    "    mask_borders=(0,150,150,50),  ## number of pixels to mask off the edges (top, bottom, left, right)\n",
    "    n_iter=50,  ## number of iterations for optimization\n",
    "    termination_eps=1e-09,  ## convergence tolerance\n",
    "    gaussFiltSize=31,  ## size of gaussian blurring filter applied to all images\n",
    "    auto_fix_gaussFilt_step=10,  ## increment in gaussFiltSize after a failed optimization\n",
    ")\n",
    "\n",
    "aligner.transform_images_geometric(ims_augmented);\n",
    "\n",
    "im_template_geo = aligner.transform_images(\n",
    "    ims_moving=[im_template],\n",
    "    remappingIdx=aligner.remappingIdx_geo,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aligner.fit_nonrigid(\n",
    "    template=im_template_geo,\n",
    "#     template=int(0),  ## specifies which image to use as the template. Either array (image), integer (ims_moving index), or float (ims_moving fractional index)\n",
    "    ims_moving=aligner.ims_registered_geo,  ## Input images. Typically the geometrically registered images\n",
    "    remappingIdx_init=aligner.remappingIdx_geo,  ## The remappingIdx between the original images (and ROIs) and ims_moving\n",
    "    template_method='image',  ## 'sequential': align images to neighboring images. 'image': align to a single image, good if using geometric registration first\n",
    "    mode_transform='createOptFlow_DeepFlow',  ## algorithm for non-rigid transformation. Either 'createOptFlow_DeepFlow' or 'calcOpticalFlowFarneback'. See openCV docs for each. \n",
    "    kwargs_mode_transform=None,  ## kwargs for `mode_transform`\n",
    ")\n",
    "\n",
    "aligner.transform_images_nonrigid(list(images.values()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.visualization.display_toggle_image_stack(aligner.ims_registered_geo)\n",
    "\n",
    "fr.visualization.display_toggle_image_stack(aligner.ims_registered_nonrigid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remappingIdx = copy.deepcopy(aligner.remappingIdx_nonrigid)\n",
    "\n",
    "points_roiBorders_transformed = [{key: aligner.transform_points(\n",
    "    points=points,\n",
    "    remappingIdx=rmap_idx,\n",
    ") for key, points in rois.roi_points.items()} for rmap_idx in remappingIdx]\n",
    "\n",
    "points_forTracking_transformed = [aligner.transform_points(\n",
    "    points=rois.point_positions,\n",
    "    remappingIdx=rmap_idx,\n",
    ") for rmap_idx in remappingIdx]\n",
    "\n",
    "exampleImages = list(images.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rois_objs_new = {name: fr.rois.ROIs(\n",
    "    select_mode='custom', \n",
    "    coords_rois=borderPoints, \n",
    "    exampleImage=exampleImage, \n",
    "    point_positions=point_positions\n",
    ") for name, borderPoints, exampleImage, point_positions in tqdm(zip(paths_videos, points_roiBorders_transformed, exampleImages, points_forTracking_transformed))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save\n",
    "Save the new `ROIs` objects. These can be used to initialize the `ROIs` objects in each face-rhythm run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names_sessions = ['__'.join(Path(name).parts[-6:-4]) for name in rois_objs_new.keys()]\n",
    "\n",
    "for ii, (name, rois_new) in enumerate(rois_objs_new.items()):\n",
    "    path_save = str(Path(directory_save) / names_sessions[ii] / f'ROIs.h5')\n",
    "#     path_save = str(Path(directory_save) / f'ROIs.h5')\n",
    "    rois_new.save_run_data(\n",
    "        path_run_data=path_save,\n",
    "        overwrite=True,\n",
    "        verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.visualization.display_toggle_image_stack(aligner.ims_registered_nonrigid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cast to uint8\n",
    "movie = [im.astype(np.uint8) for im in aligner.ims_registered_nonrigid]\n",
    "## Add text overlay\n",
    "movie = fr.helpers.add_text_to_images(movie, [[str(n)] for n in np.arange(len(movie))], position=(50,100), font_size=4, line_width=5,)\n",
    "\n",
    "image_saver = fr.util.Image_Saver(\n",
    "    dir_save=directory_save,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "image_saver.save_gif(\n",
    "    array_images=movie, \n",
    "    name_save='mouse_face_matched', \n",
    "    frame_rate=10.0, \n",
    "    loop=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_visualizer = fr.visualization.FrameVisualizer(\n",
    "    display=False,\n",
    "    frame_height_width=rois_objs_new[list(rois_objs_new.keys())[0]].img_hw,\n",
    "    point_sizes=3,\n",
    "    points_colors=(255,0,0),\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "images_with_warped_points = [frame_visualizer.visualize_image_with_points(\n",
    "    image=rois.exampleImage,\n",
    "    points=rois.point_positions,\n",
    ") for rois in rois_objs_new.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.visualization.display_toggle_image_stack(images_with_warped_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cast to uint8\n",
    "movie = [im.astype(np.uint8) for im in images_with_warped_points]\n",
    "## Add text overlay\n",
    "movie = fr.helpers.add_text_to_images(movie, [[str(n)] for n in np.arange(len(movie))], position=(50,100), font_size=4, line_width=5,)\n",
    "\n",
    "image_saver = fr.util.Image_Saver(\n",
    "    dir_save=directory_save,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "image_saver.save_gif(\n",
    "    array_images=movie, \n",
    "    name_save='mouse_face_points_matched', \n",
    "    frame_rate=3.0, \n",
    "    loop=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 957.188,
   "position": {
    "height": "40px",
    "left": "1725.67px",
    "right": "20px",
    "top": "124.914px",
    "width": "628.438px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
