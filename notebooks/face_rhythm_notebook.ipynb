{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Face Rhythm</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723283221-5Z5038AT7Y6KCOM2PIU4/Screenshot+from+2021-03-02+17-05-12.png?content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723856628-J89PYYSF7K7JATE2KMF9/Screenshot+from+2021-03-02+17-23-46.png?format=300w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723931026-OORV0RAPZNWV3R8TBOXB/Screenshot+from+2021-03-02+17-25-11.png?format=300w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614724055033-O3GBEF1D9MULFZKI2IUJ/Screenshot+from+2021-03-02+17-27-10.png?format=300w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723378405-WXN74ZTT1KYZUQGDM07X/face_rhythm_banner2.png?format=1000w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "</tr></table>\n",
    "\n",
    "***\n",
    "\n",
    "##### Notebook Shortcuts\n",
    "- **[Notebook Setup](#Notebook-Setup)**: Prepare all the necessary config files and folders\n",
    "- **[Set ROI](#Set-ROI)**: Set the ROI for the analysis\n",
    "- **[Run Optic Flow](#Run-Optic-Flow)**: Run the optic flow analysis\n",
    "- **[Clean Optic Flow](#Clean-Optic-Flow)**: Optic flow post-processing\n",
    "- **[Convolutional Dimensionality Reduction](#Convolutional-Dimensionality-Reduction)**: Convolutional Dimensionality Reduction\n",
    "- **[Analysis](#Analysis)**: Decompose and Analyze the optic flow data in many ways\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tips on running this notebook:\n",
    "In theory it would be nice if you could just enter the path of the video(s) and just let it run all the way through. In practice, there are a few hoops to jump through\n",
    "- Run the Notebook Setup Block (two blocks below this one). This should pretty much always be done, even if you are loading precomputed file from disk instead of calculating them. This step loads in some useful meta data used throughout.\n",
    "- Even if you are restarting at a specific point in your analysis, run your Setup Block then head down to your current analysis step cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Notebook Setup</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Creates config and locates videos\n",
    "\n",
    "**Crucially, always run this first cell every time you run this notebook.**\n",
    "\n",
    "Also, generally make sure to read through the config parameters before running.\n",
    "\n",
    "The Project path is the path to a folder (existing or not) where we will store our derived files. I recommend creating a project folder and then copying this notebook into that folder.\n",
    "The Video path is the path to the folder with all the raw videos. \n",
    "The run name will determine the name of the config. You might create multiple configs if you want to re-run the same data with slightly different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ALWAYS RUN THIS CELL\n",
    "# widen jupyter notebook window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "\n",
    "from face_rhythm.util import helpers, setup\n",
    "from pathlib import Path\n",
    "\n",
    "# SET THE PROJECT PATH, DATA PATH, and (optionally) run name\n",
    "project_path     = Path('./').resolve()\n",
    "video_path       = Path('../../data/').resolve()\n",
    "run_name         = 'new'\n",
    "overwrite_config = False\n",
    "remote           = False # Select true if running on a cluster (or any system where your kernel isn't on your local machine). cv2 doesn't play nicely with ssh/slurm connections, set true if using that\n",
    "trials           = False # Let us know if you're using trials and want to use them as a dimension in the factorization\n",
    "\n",
    "config_filepath = setup.setup_project(project_path, video_path, run_name, overwrite_config, remote, trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.util import helpers, setup\n",
    "# IMPORT VIDEOS\n",
    "# Video data must be organized such that data from unique sessions are in unique folders.\n",
    "# If you just have one session\n",
    "# We handle the session_prefix tag cleverly\n",
    "# If you specify a unique folder (complete path), then we'll just load just one folder (eg: 'Drive/data/session')\n",
    "# If you specify a folder prefix, then we'll load all sessions that fit that prefix (eg: 'baseline_session' where baseline_session is in the video_path)\n",
    "# If you specify no prefix, then we'll load all sessions in the video_path\n",
    "\n",
    "# %debug\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['Video']['session_prefix'] = 'session_short' \n",
    "config['Video']['print_filenames'] = True\n",
    "config['General']['overwrite_nwbs'] = False\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "setup.prepare_videos(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Set ROI</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Manually specify your roi\n",
    "\n",
    "This is good if your animal doesn't fill the view and if you have stationary objects nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from face_rhythm.util import helpers, set_roi\n",
    "\n",
    "### Select POLYGON SUBFRAME for DISPLACEMENT Eignfaces\n",
    "## This block of code will pop up a little GUI. Click around the\n",
    "## region of the face that you want to include in the analysis.\n",
    "## When you are done, click the 'Disconnect mpl' button\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['ROI']['session_to_set'] = 0 # 0 indexed. Chooses the session to use\n",
    "config['ROI']['vid_to_set'] = 0 # 0 indexed. Sets the video to use to make an image\n",
    "config['ROI']['frame_to_set'] = 1 # 0 indexed. Sets the frame number to use to make an image\n",
    "config['ROI']['load_from_file'] = False # if you've already run this and want to use the existing ROI, set to True\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "frame, bbox_selector = set_roi.get_roi(config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this until you're done selecting\n",
    "if not config['ROI']['load_from_file']:\n",
    "    set_roi.process_roi(config_filepath, frame, bbox_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Run Optic Flow</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "# Optic flow calculation\n",
    "\n",
    "Multithread is generally 2X to many-X faster, but may fail when too many dots are selected (memory overload)\n",
    "\n",
    "*If show video set to true on a remote connection, the video won't show, but it will save to the proj folder.*\n",
    "\n",
    "Key Optic flow params:\n",
    "- **'spacing'**: ~ 3 to 12. Spacing between dots, in pixels. Inversely related to number of dots to use in the calculation. Try to keep the number of dots below 2000 if possible (eats up memory and computation time). More dots generally means better final results, more robust to outliers and weird stuff. I'd make the spacing as small (more dots) as you can go before you run out of RAM in the final calculations\n",
    "- **lk_params 'win_size'**: ~ 25,25 to 80,80. This is the spatial integration window for the optical flow measurement. Try to make it as small as possible without it becoming unstable. The two values are for X and Y length of square integration window. Probably keep the same for most applications\n",
    "- **multithread**: There are two ways to run optic flow: one single-threaded and one multi-threaded. Do parameter tuning on the single-threaded one so you can quit out of it, as well as watch the calculation as it happens with showVideo_pref=True. The multi-threaded one is only faster if you have a lot of cores in your CPU (>10), then it's faster, else stick with the single-threaded version and set showVideo_pref=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from face_rhythm.optic_flow import optic_flow\n",
    "import cv2\n",
    "\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "config['Optic']['vidNums_toUse'] = list(range(config['General']['sessions'][0]['num_vids'])) ## 0 indexing. Use this line of code to run all the videos in a particular session\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "# win size: spatial integration window (make small as possible, but make bigger if youre having issues with outliers)\n",
    "# max level: only moderate effects if everything working properly. Keep around 3.\n",
    "# criteria values have to do with the search algorithm. For speed: EPS small, COUNT big (if data is gud)\n",
    "config['Optic']['spacing'] = 8  ## This is the distance between points in the grid (both in x and y dims)\n",
    "config['Optic']['lk'] = {}\n",
    "config['Optic']['lk']['winSize']     = (15,15)\n",
    "config['Optic']['lk']['maxLevel']    = 2\n",
    "config['Optic']['lk']['criteria']    = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 3, 0.001)\n",
    "\n",
    "\n",
    "config['Optic']['showVideo_pref'] = False  ## much faster when video is off. If 'remote' option chosen (from first cell block), video will be saved as file in project folder.\n",
    "config['Video']['dot_size'] = 1  ## for viewing purposes\n",
    "config['Video']['save_demo'] = False # Whether to save the demo video (true for remote users when showvideo is true)\n",
    "config['Video']['demo_len'] = 10 # used when remote users when show_video==True\n",
    "\n",
    "config['Optic']['recursive'] = True\n",
    "config['Optic']['recursive_relaxation_factor'] = 0.005\n",
    "config['Optic']['multithread'] = False # Must be False if 'recursive'==True  OR  if 'showVideo_pref'==True\n",
    "\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "\n",
    "### == CALCULATION ==\n",
    "\n",
    "optic_flow.optic_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Clean Optic Flow</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Clean up displacements traces and make good positions traces\n",
    "\n",
    "Key Outlier removal params:\n",
    "- **outlier_threshold_positions**: ~ 20 to 100. If a dot strays more than this many pixels away from its anchor position, its displacement in the dimension it cross the threshold in, for those time points (and some time points around it, see params below), for that dot only, will be set to zero\n",
    "- **outlier_threshold_displacements** ~ 5 to 25. Similar to above, but for displacement. Only the outlier time points are removed (no window around outliers considered).\n",
    "- **framesHalted_beforeOutlier**: ~ 0 to 30. The number of frames to also remove before detected outlier events. Consider what is causing your outlier event. If it is an arm movement or something, how long does such a movement last? How long before it will cause a dot to move to the outlier threshold?\n",
    "- **framesHalted_afterOutlier**: ~ 0 to 10. Simlar to above but for after an outlier event is detected\n",
    "- **relaxation_factor** : ~ 0.03 to 0.1. This is the rate of the exponential decay / relaxation / attraction back to the anchor position that a point undergoes. It is meant to prevent baseline drift. Think of it like a high pass on the dot position trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from face_rhythm.optic_flow import clean_results\n",
    "\n",
    "## Create position trace from displacements\n",
    "## This block does a few things:\n",
    "\n",
    "## 1. Finds outliers: These are currently defined as time points when the integrated position goes beyond some threshold.\n",
    "##  Note that since displacements are calculated for x and y separately, outlier events are also separated into x outlier events\n",
    "##  and y outlier events.\n",
    "\n",
    "## 2. Sets displacements during outlier events to ZERO: There are some parameters below that define the time window (in frames)\n",
    "##  before and after outliers to also set to zero. Note again, that DISPLACEMENT (the derivative of position) is set to zero, \n",
    "##  effectively pausing the position of the ingegrated position.\n",
    "\n",
    "## 3. Rectifies the position to its 'anchor position': I am defining position as the integrated displacement arising from a STATIC\n",
    "##  place in the image. Because this analysis is image agnostic, drift naturally occurs. This term counteracts drift by simply\n",
    "##  relaxing each dot's position back to the center of its displacement analysis window. This term should be as low as possible\n",
    "##  because it also acts as a high pass filter, thus precluding analysis of slow timescale changes.\n",
    "\n",
    "## Note that using a standard frequency filter (fir, iir) here for the rectification / relaxation doesn't work well\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['Clean']['outlier_threshold_positions'] = 25 ## in pixels. If position goes past this, short time window before and including outlier timepoint has displacement set to 0 \n",
    "config['Clean']['outlier_threshold_displacements'] = 4 ## in pixels. If displacement goes past this, displacement set to 0 at those time points\n",
    "config['Clean']['framesHalted_beforeOutlier'] = 4 # in frames. best to make even\n",
    "config['Clean']['framesHalted_afterOutlier'] = 2 # in frames. best to make even\n",
    "config['Clean']['relaxation_factor'] = 0.005 # This is the speed at which the integrated position exponentially relaxes back to its anchored position. Make ~0.005 to 0.05 for face video at 120 Hz\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "clean_results.clean_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new points!\n",
    "from face_rhythm.visualize import videos\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['Video']['demo_len'] = 100\n",
    "config['Video']['data_to_display'] = 'positions_absolute'\n",
    "config['Video']['save_demo'] = False \n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "videos.visualize_points(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Convolutional Dimensionality Reduction</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Do some denoising and to get the number of dots down to a managable number\n",
    "\n",
    "In particular, it is nice for the batched CP decomposition later that the batches can be as big as possible in the temporal dimension, so doing some mild convolutional dim reduction first is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from face_rhythm.optic_flow import conv_dim_reduce\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "pointInds_toUse = helpers.load_data(config_filepath, 'pointInds_toUse')\n",
    "\n",
    "# Create kernel\n",
    "config['CDR']['width_cosKernel'] = 24 # This is the radius of a 2-dimensional cosine kernel. If you get an error about SVD not working, probably increase this\n",
    "config['CDR']['num_dots'] = pointInds_toUse.shape[0]\n",
    "\n",
    "# Distance between points in the grid, longer than optic \n",
    "config['CDR']['spacing'] = 8\n",
    "\n",
    "# For displaying dots\n",
    "config['CDR']['display_points'] = False # checkout the dots and overlayed filter\n",
    "config['CDR']['vidNum'] = 0 # 0 indexed\n",
    "config['CDR']['frameNum'] = 1 # 0 indexed\n",
    "config['CDR']['dot_size'] = 1\n",
    "config['CDR']['kernel_alpha'] = 0.3\n",
    "config['CDR']['kernel_pixel'] = 10\n",
    "\n",
    "# Coefficients of influence \n",
    "config['CDR']['num_components'] = 3\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "conv_dim_reduce.conv_dim_reduce_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new points!\n",
    "\n",
    "from face_rhythm.visualize import videos\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['Video']['demo_len'] = 100\n",
    "config['Video']['data_to_display'] = 'positions_convDR_absolute'\n",
    "config['Video']['save_demo'] = True \n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "videos.visualize_points(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Analysis</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Decompose and Analyze the Data in different ways\n",
    "\n",
    "Below you'll find the following:\n",
    "- PCA done on the point positions\n",
    "- TCA done on the point positions\n",
    "- Spectral analysis of every pixel to transoform the basis to be oscillatory\n",
    "- TCA done on the spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import pca\n",
    "\n",
    "pca.pca_workflow(config_filepath, 'positions_convDR_absolute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from face_rhythm.visualize import plots\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['PCA']['n_factors_to_show'] = 3\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "plots.plot_pca_diagnostics(config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_rhythm.visualize import videos\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "config['Video']['factor_category_to_display'] = 'PCA' # eg: 'TCA' or 'PCA'\n",
    "config['Video']['factor_to_display'] = 'factors_points' # eg: (if 'TCA'):'factors_frequential_points' (if 'PCA'):'factors_points'\n",
    "config['Video']['points_to_display'] = 'positions_convDR_absolute' # eg: 'positions_convDR_absolute' or 'positions_absolute' or 'positions_recursive'\n",
    "config['Video']['demo_len'] = 100\n",
    "config['Video']['dot_size'] = 2\n",
    "config['Video']['save_demo'] = True \n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "videos.visualize_factor(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional TCA\n",
    "\n",
    "Key TCA parameters:\n",
    "- **device**: runs a small function to get your device. Set to true if you want to use the GPU. If the input is small (< half the size of your GPU memory), set to True. It's super fast.\n",
    "- **rank**: ~ 2 to 10. The number of factors to look for in the PARAFAC model. More can be good but less reproduceable, but less can mix together obviously different factors\n",
    "- **tolerance**: ~1e-05 to 1e-07. The minimum variance of the model between steps before declaring convergence. If you're trying to really optimize your TCA fit, try decreasing this. \n",
    "- **n_iters**: ~100 to 600. The fit of the model usually improves if you give it more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import tca\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['TCA']['device'] = tca.use_gpu(False)\n",
    "config['TCA']['rank'] = 4\n",
    "config['TCA']['init'] = 'random'\n",
    "config['TCA']['tolerance'] = 1e-06 \n",
    "config['TCA']['verbosity'] = 0\n",
    "config['TCA']['n_iters'] = 500 \n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "tca.positional_tca_workflow(config_filepath, 'positions_convDR_meanSub') # you can use different positions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from face_rhythm.visualize import plots\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['TCA']['ftype'] = 'positional'\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "plots.plot_tca_factors(config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_rhythm.visualize import videos\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "config['Video']['factor_category_to_display'] = 'TCA' # eg: 'TCA' or 'PCA'\n",
    "config['Video']['factor_to_display'] = 'factors_positional_points' # eg: (if 'TCA'):'factors_frequential_points' (if 'PCA'):'scores_points'\n",
    "config['Video']['points_to_display'] = 'positions_absolute' # eg: 'positions_convDR_absolute' or 'positions_absolute' or 'positions_recursive'\n",
    "config['Video']['demo_len'] = 100\n",
    "config['Video']['dot_size'] = 2\n",
    "config['Video']['save_demo'] = True \n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "videos.visualize_factor(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Analysis\n",
    "I've played with a few different methods. While multiresolution methods seems ideal for this use-case, It just ends up severly overrepresenting low frequency factors, making noisier high frequency factors, and doing an overall worse job at reconstruction.\n",
    "A good ol' multitaper short time fourier transform seems to work fine. Adding in raw positions to subsequent dimensionality reduction later on seems like a natural thing to do, as single resolution spectral analysis ends up kind of ignoring slower dynamics.\n",
    "\n",
    "We recommend running the first cell to just visualize and assess the frequencies you're using. You can change this frequency distribution by altering the provided parameters \n",
    "\n",
    "Key Spectral analysis params:\n",
    "- **hop_length**: ~ 5 to 20. The length of the time window used for the short-time Fourier transform. Longer gives better spectral resolution, shorter gives better temporal resolution. There are several other parameters that are related but this is the most important. Longer windows (along with decreasing the overlap parameter) also decrease the size of the output spectrograms, which can help with memory and computation time in the subsequent analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import spectral_analysis\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['CQT']['hop_length'] = 16\n",
    "config['CQT']['fmin_rough'] = 1.8\n",
    "config['CQT']['sampling_rate'] = config['Video']['Fs']\n",
    "config['CQT']['n_bins'] = 35\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "spectral_analysis.prepare_freqs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import spectral_analysis\n",
    "\n",
    "spectral_analysis.cqt_workflow(config_filepath, 'positions_convDR_meanSub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from face_rhythm.visualize import plots\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['CQT']['pixelNum_toUse'] = 1\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "plots.plot_cqt(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TCA\n",
    "\n",
    "Key TCA parameters:\n",
    "- **device**: runs a small function to get your device. Set to true if you want to use the GPU. If the input is small (< half the size of your GPU memory), set to True. It's super fast.\n",
    "- **rank**: ~ 2 to 10. The number of factors to look for in the PARAFAC model. More can be good but less reproduceable, but less can mix together obviously different factors\n",
    "- **tolerance**: ~1e-05 to 1e-07. The minimum variance of the model between steps before declaring convergence. If you're trying to really optimize your TCA fit, try decreasing this. \n",
    "- **n_iters**: ~100 to 600. The fit of the model usually improves if you give it more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import tca\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['TCA']['device'] = tca.use_gpu(False)\n",
    "config['TCA']['rank'] = 8\n",
    "config['TCA']['init'] = 'random' \n",
    "config['TCA']['tolerance'] = 1e-06 \n",
    "config['TCA']['verbosity'] = 0\n",
    "config['TCA']['n_iters'] = 100 \n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "tca.full_tca_workflow(config_filepath, 'positions_convDR_meanSub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from face_rhythm.visualize import plots\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['TCA']['ftype'] = 'spectral'\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "plots.plot_tca_factors(config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_rhythm.visualize import videos\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "config['Video']['factor_category_to_display'] = 'TCA' # eg: 'TCA' or 'PCA'\n",
    "config['Video']['factor_to_display'] = 'factors_spectral_points' # eg: (if 'TCA'):'factors_frequential_points' (if 'PCA'):'scores_points'\n",
    "config['Video']['points_to_display'] = 'positions_convDR_absolute' # eg: 'positions_convDR_absolute' or 'positions_absolute' or 'positions_recursive'\n",
    "config['Video']['demo_len'] = 100\n",
    "config['Video']['dot_size'] = 2\n",
    "config['Video']['save_demo'] = True \n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "videos.visualize_factor(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END analysis\n",
    "\n",
    "Below are some examples on how to access, plot, and manipulate the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs:\n",
    "Below is the output tree structure of the NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = helpers.load_config(config_filepath)\n",
    "nwb_path = config['General']['sessions'][0]['nwb']\n",
    "helpers.dump_nwb(nwb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: How to plot factors: Frequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import sklearn.decomposition\n",
    "import pynwb\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "nwb_path = config['General']['sessions'][0]['nwb']\n",
    "\n",
    "freqs_Sxx = np.load(config['Paths']['freqs_Sxx'])\n",
    "Fs = config['Video']['Fs']\n",
    "\n",
    "with pynwb.NWBHDF5IO(nwb_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    freq_components = nwbfile.processing['Face Rhythm']['TCA']['factors_frequential_frequential'].data\n",
    "    dot_components = nwbfile.processing['Face Rhythm']['TCA']['factors_frequential_points'].data\n",
    "    temp_components_interp = nwbfile.processing['Face Rhythm']['TCA']['factors_frequential_temporal_interp'].data\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(freqs_Sxx , freq_components[:,:])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('frequency (Hz)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(dot_components[:,:])\n",
    "    plt.xlabel('dotIDs (concat X then Y)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(temp_components_interp.shape[0])/Fs , temp_components_interp[:,:])\n",
    "    plt.xlabel('time (s)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(np.corrcoef(np.array(temp_components_interp).T) - np.eye(8),aspect='auto')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    pca = sklearn.decomposition.PCA(n_components=8)\n",
    "    pca.fit(temp_components_interp)\n",
    "#     PCA(n_components=8)\n",
    "    plt.figure()\n",
    "    plt.plot(pca.explained_variance_ratio_)\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('explained variance ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: How to plot factors: Positional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.decomposition\n",
    "import pynwb\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "nwb_path = config['General']['sessions'][0]['nwb']\n",
    "\n",
    "freqs_Sxx = np.load(config['Paths']['freqs_Sxx'])\n",
    "Fs = config['Video']['Fs']\n",
    "\n",
    "with pynwb.NWBHDF5IO(nwb_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    dot_components = np.array(nwbfile.processing['Face Rhythm']['TCA']['factors_positional_points'].data)\n",
    "    temp_components_interp = np.array(nwbfile.processing['Face Rhythm']['TCA']['factors_frequential_temporal_interp'].data)\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.plot(freqs_Sxx , freq_components[:,:])\n",
    "#     plt.xscale('log')\n",
    "#     plt.xlabel('frequency (Hz)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(dot_components[:,:])\n",
    "    plt.xlabel('dotIDs (concat X then Y)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(temp_components_interp.shape[0])/Fs , temp_components_interp[:,:])\n",
    "    plt.xlabel('time (s)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(np.corrcoef(np.array(temp_components_interp).T) - np.eye(8),aspect='auto')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    pca = sklearn.decomposition.PCA(n_components=8)\n",
    "    pca.fit(temp_components_interp)\n",
    "#     PCA(n_components=8)\n",
    "    plt.figure()\n",
    "    plt.plot(pca.explained_variance_ratio_)\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('explained variance ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: How to access NWB output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynwb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "nwb_path = config['General']['sessions'][0]['nwb']\n",
    "with pynwb.NWBHDF5IO(nwb_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    # look through the NWB outputs (see above example) to see available arrays to plot and how to access them\n",
    "    example_data = nwbfile.processing['Face Rhythm']['Optic Flow']['positions'].data\n",
    "    print(example_data)\n",
    "    plt.figure()\n",
    "    plt.plot(example_data[0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to access parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = helpers.load_config(config_filepath)\n",
    "print(config['lk_winSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:face-rhythm] *",
   "language": "python",
   "name": "conda-env-face-rhythm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 957.188,
   "position": {
    "height": "40px",
    "left": "1725.67px",
    "right": "20px",
    "top": "124.914px",
    "width": "628.438px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
