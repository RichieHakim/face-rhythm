{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Face Rhythm</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1wcz_8-cq6MppJ_wkOOu7GkeZkYI3uGB1\">\n",
    "\n",
    "***\n",
    "\n",
    "##### Notebook Shortcuts\n",
    "- **[Notebook Setup](#Notebook-Setup)**: Prepare all the necessary config files and folders\n",
    "- **[Set ROI](#Set-ROI)**: Set the ROI for the analysis\n",
    "- **[Run Optic Flow](#Run-Optic-Flow)**: Run the optic flow analysis\n",
    "- **[Clean Optic Flow](#Clean-Optic-Flow)**: Optic flow post-processing\n",
    "- **[Convolutional Dimensionality Reduction](#Convolutional-Dimensionality-Reduction)**: Convolutional Dimensionality Reduction\n",
    "- **[Analysis](#Analysis)**: Decompose and Analyze the optic flow data in many ways\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tips on running this notebook:\n",
    "In theory it would be nice if you could just enter the path of the video(s) and just let it run all the way through. In practice, there are a few hoops to jump through\n",
    "- Run the Notebook Setup Block (two blocks below this one). This should pretty much always be done, even if you are loading precomputed file from disk instead of calculating them. This step loads in some useful meta data used throughout.\n",
    "- Even if you are restarting at a specific point in your analysis, run your Setup Block then head down to your current analysis step cell \n",
    "- There are two options for calculating the optic flow (parameters set independently as well), one single-threaded and one multi-threaded. Do parameter tuning on the single-threaded one so you can quit out of it, as well as watch the calculation as it happens with showVideo_pref=1. The multi-threaded one is only faster if you have a lot of cores in your CPU (>10), then it's faster, else stick with the single-threaded version and set showVideo_pref=0.\n",
    "\n",
    "### The most important parameters:  \n",
    "***(Consider all of these before you run the code for the first time)***\n",
    "- Optic flow params:\n",
    "    - **'spacing'**: ~ 3 to 12. Spacing between dots, in pixels. Inversely related to number of dots to use in the calculation. Try to keep the number of dots below 2000 if possible (eats up memory and computation time). More dots generally means better final results, more robust to outliers and weird stuff. I'd make the spacing as small (more dots) as you can go before you run out of RAM in the final calculations\n",
    "    - **lk_params 'win_size'**: ~ 25,25 to 80,80. This is the spatial integration window for the optical flow measurement. Try to make it as small as possible without it becoming unstable. The two values are for X and Y length of square integration window. Probably keep the same for most applications\n",
    "- Outlier removal params:\n",
    "    - **outlier_threshold_positions**: ~ 20 to 100. If a dot strays more than this many pixels away from its anchor position, its displacement in the dimension it cross the threshold in, for those time points (and some time points around it, see params below), for that dot only, will be set to zero\n",
    "    - **outlier_threshold_displacements** ~ 5 to 25. Similar to above, but for displacement. Only the outlier time points are removed (no window around outliers considered).\n",
    "    - **framesHalted_beforeOutlier**: ~ 0 to 30. The number of frames to also remove before detected outlier events. Consider what is causing your outlier event. If it is an arm movement or something, how long does such a movement last? How long before it will cause a dot to move to the outlier threshold?\n",
    "    - **framesHalted_afterOutlier**: ~ 0 to 10. Simlar to above but for after an outlier event is detected\n",
    "    - **relaxation_factor** : ~ 0.03 to 0.1. This is the rate of the exponential decay / relaxation / attraction back to the anchor position that a point undergoes. It is meant to prevent baseline drift. Think of it like a high pass on the dot position trace\n",
    "- Spectral analysis params:\n",
    "    - **win_len**: ~ 0.1 to 1.0. The length of the time window used for the short-time Fourier transform. Longer gives better spectral resolution, shorter gives better temporal resolution. There are several other parameters that are related but this is the most important. Longer windows (along with decreasing the overlap parameter) also decrease the size of the output spectrograms, which can help with memory and computation time in the subsequent analyses\n",
    "- TCA:\n",
    "    - **rank = 6**: ~ 2 to 10. The number of factors to look for in the PARAFAC model. More can be good but less reproduceable, but less can mix together obviously different factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Notebook Setup</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Creates config and locates videos\n",
    "\n",
    "**Crucially, always run this first cell every time you run this notebook.**\n",
    "\n",
    "Also, generally make sure to read through the config parameters before running.\n",
    "\n",
    "The Project path is the path to a folder (existing or not) where we will store our derived files. I recommend creating a project folder and then copying this notebook into that folder.\n",
    "The Video path is the path to the folder with all the raw videos. \n",
    "The session name will determine the name of the config. You might create multiple configs if you want to re-run the same data with slightly different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ALWAYS RUN THIS CELL\n",
    "# widen jupyter notebook window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "\n",
    "# Import libraries\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.cuda\n",
    "\n",
    "from face_rhythm.util import helpers\n",
    "from pathlib import Path\n",
    "\n",
    "# SET THE PROJECT PATH, DATA PATH, and (optionally) session name\n",
    "project_path     = Path('./').resolve()\n",
    "video_path        = Path('../../original_recordings/gmou6/082720/').resolve()\n",
    "session_name     = 'test'\n",
    "overwrite_config = False\n",
    "\n",
    "config_filepath = helpers.setup_project(project_path, video_path, session_name, overwrite_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORT VIDEOS\n",
    "# This option imports all of the videos with a defined file name prefix in a folder\n",
    "# OR just imports a single defined file. It uses a naming convention / file pattern match, so be careful\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "config['multiple_files_pref'] = True\n",
    "\n",
    "# Used only if 'multiple_files_pref'==1\n",
    "config['fileName_vid_prefix'] = 'gmou06_082720_faceTrack_session1_short' \n",
    "config['fileName_vid_numDigitsInIteration'] = 4 # number of digits in the movie (used when movie is broken up into chunks)\n",
    "config['fileName_vid_suffix'] = '.avi'\n",
    "\n",
    "# Used only if 'multiple_files_pref'==0\n",
    "config['fileName_vid'] = 'gmou06_082720_faceTrack_session1_crop.avi'\n",
    "\n",
    "config['print_fileNames_pref'] = 1\n",
    "\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "helpers.import_videos(config_filepath)\n",
    "helpers.get_video_data(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Set ROI</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Manually specify your roi\n",
    "\n",
    "This is good if your animal doesn't fill the view and if you have stationary objects nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from face_rhythm.util import helpers\n",
    "from face_rhythm.util import set_roi\n",
    "\n",
    "### Select POLYGON SUBFRAME for DISPLACEMENT Eignfaces\n",
    "## This block of code will pop up a little GUI. Click around the\n",
    "## region of the face that you want to include in the analysis.\n",
    "## When you are done, press enter twice to accept and exit the GUI.\n",
    "\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['vidToSet'] = 1 # 1 indexed. Sets the video to use to make an image\n",
    "config['frameToSet'] = 2 # 1 indexed. Sets the frame number to use to make an image\n",
    "config['load_from_file'] = True\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "set_roi.roi_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Run Optic Flow</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Run as either mono or multi threaded depending on run time and number of dots\n",
    "\n",
    "Multithread may struggle when too many dots are selected (memory overload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.optic_flow import optic_flow\n",
    "\n",
    "## Important assumptions about using this code verses the single (ish) threaded version:\n",
    "## 1. numFrames (per file) + 1000 (USING OPENCVs cv2.CAP_PROP_FRAME_COUNT) must be > true number of frames\n",
    "##  as found using the imageio import method\n",
    "## 2. Debugging is hard. If you interrupt the kernel while it's doing the parallel pool, the kernel goofs\n",
    "##  and generally requires a restart\n",
    "\n",
    "### == PREFERENCES ==\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "config['vidNums_toUse'] = list(range(config['numVids'])) ## 0 indexing\n",
    "config['spacing'] = 8  ## This is the distance between points in the grid (both in x and y dims)\n",
    "\n",
    "config['showVideo_pref'] = False  ## much faster when video is off\n",
    "config['dot_size'] = 1  ## for viewing purposes\n",
    "\n",
    "## below will print the fps ever 'fps_counterPeriod' . Useful for checking the speed of import.\n",
    "## Best to turn off when doing a full run. (this is mostly for optimizing and debugging)\n",
    "config['printFPS_pref'] = False\n",
    "config['fps_counterPeriod'] = 10 ## number of frames to do a tic toc over\n",
    "\n",
    "## Parameters for lucas kanade optical flow\n",
    "## win size: spatial integration window (make small as possible, but make bigger if youre having issues with outliers)\n",
    "## max level: only moderate effects if everything working properly. Keep around 3.\n",
    "## criteria values have to do with the search algorithm. For speed: EPS small, COUNT big.\n",
    "config['lk_winSize']  = (80,80)\n",
    "config['lk_maxLevel'] = 4\n",
    "config['lk_criteria']    = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 100, 0.001)\n",
    "\n",
    "config['optic_multithread'] = True\n",
    "\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "\n",
    "### == CALCULATION ==\n",
    "\n",
    "optic_flow.optic_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Clean Optic Flow</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Clean up displacements traces and make good positions traces\n",
    "\n",
    "Check the parameters here, they are essential for getting good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.optic_flow import clean_results\n",
    "\n",
    "## Create position trace from displacements\n",
    "## This block does a few things:\n",
    "\n",
    "## 1. Finds outliers: These are currently defined as time points when the integrated position goes beyond some threshold.\n",
    "##  Note that since displacements are calculated for x and y separately, outlier events are also separated into x outlier events\n",
    "##  and y outlier events.\n",
    "\n",
    "## 2. Sets displacements during outlier events to ZERO: There are some parameters below that define the time window (in frames)\n",
    "##  before and after outliers to also set to zero. Note again, that DISPLACEMENT (the derivative of position) is set to zero, \n",
    "##  effectively pausing the position of the ingegrated position.\n",
    "\n",
    "## 3. Rectifies the position to its 'anchor position': I am defining position as the integrated displacement arising from a STATIC\n",
    "##  place in the image. Because this analysis is image agnostic, drift naturally occurs. This term counteracts drift by simply\n",
    "##  relaxing each dot's position back to the center of its displacement analysis window. This term should be as low as possible\n",
    "##  because it also acts as a high pass filter, thus precluding analysis of slow timescale changes.\n",
    "\n",
    "## Note that using a standard frequency filter (fir, iir) here for the rectification / relaxation doesn't work well\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['outlier_threshold_positions'] = 50 ## in pixels. If position goes past this, short time window before and including outlier timepoint has displacement set to 0 \n",
    "config['outlier_threshold_displacements'] = 7 ## in pixels. If displacement goes past this, displacement set to 0 at those time points\n",
    "config['framesHalted_beforeOutlier'] = 10 # in frames. best to make even\n",
    "config['framesHalted_afterOutlier'] = 5 # in frames. best to make even\n",
    "config['relaxation_factor'] = 0.003 # in frames. best to make even\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "clean_results.clean_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Convolutional Dimensionality Reduction</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Do some denoising and to get the number of dots down to a managable number\n",
    "\n",
    "In particular, it is nice for the batched CP decomposition later that the batches can be as big as possible in the temporal dimension, so doing some mild convolutional dim reduction first is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from face_rhythm.optic_flow import conv_dim_reduce\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "pointInds_toUse = helpers.load_data(config_filepath, 'path_pointInds_toUse')\n",
    "\n",
    "# Create kernel\n",
    "config['cdr_width_cosKernel'] = 25\n",
    "config['cdr_num_dots'] = pointInds_toUse.shape[0]\n",
    "\n",
    "# Distance between points in the grid\n",
    "config['cdr_spacing'] = 8 \n",
    "\n",
    "# For displaying dots\n",
    "config['cdr_vidNum'] = 0 # 0 indexed\n",
    "config['cdr_frameNum'] = 0 # 0 indexed\n",
    "config['cdr_dot_size'] = 1\n",
    "\n",
    "# Coefficients of influence \n",
    "config['cdr_num_components'] = 4\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "conv_dim_reduce.conv_dim_reduce_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>Analysis</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Decompose and Analyze the Data in different ways\n",
    "\n",
    "Below you'll find the following:\n",
    "- PCA done on raw positions\n",
    "- Spectral analysis of every pixels to transoform the basis to be oscillatory\n",
    "- TCA done on the spectra\n",
    "- A lonely t-SNE plot of the temporal factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "The X and Y displacements are concatenated and run together. Something interesting to try would be to transform to polar coordinates, concatenate and run that way. Maybe TCA on the positions with magnitude vs angle being one of the dimensions would make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import pca\n",
    "\n",
    "pca.pca_workflow(config_filepath, 'path_positions_convDR_absolute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional TCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import tca\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Prepare Tensorly\n",
    "# If the input is small ( < half the size of your GPU memory) and you have CUDA, set to 'gpu'. It's super fast.\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['tca_pref_useGPU'] = False\n",
    "\n",
    "if config['tca_pref_useGPU']:\n",
    "    cuda_device_number = torch.cuda.current_device()\n",
    "    print(f\"using CUDA device: 'cuda:{cuda_device_number}'\")\n",
    "    device = f'cuda:{cuda_device_number}'\n",
    "else:\n",
    "    print(f\"using CPU\")\n",
    "    device = 'cpu'  \n",
    "\n",
    "config['tca_device'] = device\n",
    "config['tca_rank'] = 4\n",
    "config['tca_init'] = 'random'\n",
    "config['tca_vid_display'] = True\n",
    "config['tca_vid_save'] = True\n",
    "positional_path = project_path / 'viz' / 'positional'\n",
    "positional_path.mkdir(parents=True, exist_ok=True)\n",
    "config['tca_vid_dir'] = str(positional_path)\n",
    "config['tca_verbosity'] = 0\n",
    "config['tca_n_iters'] = 100\n",
    "config['tca_display_frames'] = 500\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "tca.positional_tca_workflow(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Analysis\n",
    "I've played with a few different methods. While multiresolution methods seems ideal for this use-case, It just ends up severly overrepresenting low frequency factors, making noisier high frequency factors, and doing an overall worse job at reconstruction.\n",
    "A good ol' multitaper short time fourier transform seems to work fine. Adding in raw positions to subsequent dimensionality reduction later on seems like a natural thing to do, as single resolution spectral analysis ends up kind of ignoring slower dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Prepare Tensorly\n",
    "# If the input is small ( < half the size of your GPU memory) and you have CUDA, set to 'gpu'. It's super fast.\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "eps = 1.19209e-07 # float32 epsilon\n",
    "\n",
    "hop_length = 16\n",
    "fmin_rough = 2\n",
    "Fs = config['vid_Fs']\n",
    "sr = Fs\n",
    "n_bins = 50\n",
    "bins_per_octave = int(np.round((n_bins) / np.log2( (Fs/2)/fmin_rough )))\n",
    "fmin = ( (Fs/2)/(2**((n_bins)/bins_per_octave)) ) - (2*eps)\n",
    "fmax = fmin*(2**((n_bins)/bins_per_octave))\n",
    "\n",
    "freqs_Sxx = fmin*(2**((np.arange(n_bins)+1)/bins_per_octave))\n",
    "\n",
    "print(f'bins_per_octave: {round(bins_per_octave)} bins/octave')\n",
    "print(f'minimum frequency (fmin): {round(fmin,3)} Hz')\n",
    "print(f'maximum frequency (fmax): {round(fmax,8)} Hz')\n",
    "print(f'Nyquist                 : {sr/2} Hz')\n",
    "print(f'number of frequencies:    {n_bins} bins')\n",
    "plt.figure()\n",
    "plt.plot(freqs_Sxx)\n",
    "print(f'Frequencies: {np.round(freqs_Sxx , 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import spectral_analysis\n",
    "\n",
    "### Parameters for multitaper short-time Fourier transform\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['cqt_hop_length'] = hop_length\n",
    "config['cqt_sr'] = sr\n",
    "config['cqt_n_bins'] = n_bins\n",
    "config['cqt_bins_per_octave'] = bins_per_octave\n",
    "config['cqt_fmin'] = fmin\n",
    "config['cqt_fmin_rough'] = fmin_rough\n",
    "config['cqt_fmax'] = fmax\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "helpers.save_data(config_filepath, 'freqs_Sxx', freqs_Sxx)\n",
    "\n",
    "# CQT spectrogram for every pixel\n",
    "# this code was previously parallelized, \n",
    "# but it's pretty quick compared to the other steps, \n",
    "# so might as well keep it simple\n",
    "\n",
    "spectral_analysis.cqt_all(config_filepath, 'path_positions_convDR_meanSub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import spectral_analysis\n",
    "\n",
    "# Positional CQT\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "config['cqt_hop_length'] = 8\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "spectral_analysis.cqt_positions(config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TCA\n",
    "There are two major tensor packages, one is tensortools (made by an acquaintance named Alex Williams) and the other is Tensorly.\n",
    "Tensorly seems to be more packaged up and has some options to use some advanced backends like torch, tf, and mxnet. Though there are\n",
    "a couple of nice features in tensortools that Tensorly doesn't have, though. Generally tensortools gives better reconstructions, but takes\n",
    "much longer to do it.\n",
    "\n",
    "This function also spits out a ton of plots, consult Rich or Akshay for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_rhythm.analysis import tca\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "config = helpers.load_config(config_filepath)\n",
    "\n",
    "### Prepare Tensorly\n",
    "\n",
    "# If the input is small ( < half the size of your GPU memory) and you have CUDA, set to 'gpu'. It's super fast.\n",
    "\n",
    "config['tca_pref_useGPU'] = False\n",
    "\n",
    "if config['tca_pref_useGPU']:\n",
    "    cuda_device_number = torch.cuda.current_device()\n",
    "    print(f\"using CUDA device: 'cuda:{cuda_device_number}'\")\n",
    "    device = f'cuda:{cuda_device_number}'\n",
    "else:\n",
    "    print(f\"using CPU\")\n",
    "    device = 'cpu'  \n",
    "\n",
    "config['tca_device'] = device\n",
    "config['tca_rank'] = 8\n",
    "config['tca_vid_display'] = True\n",
    "config['tca_vid_save'] = True\n",
    "path_frequential = project_path / 'viz' / 'frequential'\n",
    "path_frequential.mkdir(parents=True, exist_ok=True)\n",
    "config['tca_vid_dir'] = str(path_frequential)\n",
    "config['tca_verbosity'] = 0\n",
    "config['tca_n_iters'] = 100\n",
    "config['tca_display_frames'] = 500\n",
    "helpers.save_config(config, config_filepath)\n",
    "\n",
    "tca.full_tca_workflow(config_filepath, 'path_positions_convDR_absolute')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:face-rhythm] *",
   "language": "python",
   "name": "conda-env-face-rhythm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 957.188,
   "position": {
    "height": "40px",
    "left": "1725.67px",
    "right": "20px",
    "top": "124.914px",
    "width": "628.438px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
